# ArtifactKeeper Full Stack - Kubernetes Manifest
# Namespace: artifactkeeper-1
# Generated from docker-compose.local-dev.yml reference
#
# Deploys: PostgreSQL, Meilisearch, Trivy, DependencyTrack, Backend, Web Frontend
# All PVs use hostPath under /home/khan/ak-data/artifactkeeper-1/
#
# NodePort assignments (Tailscale IP: 100.94.78.107):
#   Web UI:           30100
#   Meilisearch:      30170
#   Backend API:      30180
#   Backend gRPC:     30190
#   DependencyTrack:  30192

# =============================================================================
# Namespace
# =============================================================================
apiVersion: v1
kind: Namespace
metadata:
  name: artifactkeeper-1
  labels:
    app.kubernetes.io/part-of: artifact-keeper

---
# =============================================================================
# ConfigMap: init-db.sql (PostgreSQL init script)
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-init-db
  namespace: artifactkeeper-1
data:
  init-db.sql: |
    -- Initialize additional databases for services
    -- This script runs once when the PostgreSQL container is first created

    -- Create Dependency-Track database
    CREATE DATABASE dependency_track;
    GRANT ALL PRIVILEGES ON DATABASE dependency_track TO registry;

---
# =============================================================================
# ConfigMap: init-dtrack.sh (DependencyTrack bootstrap script)
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: dtrack-init-script
  namespace: artifactkeeper-1
data:
  init-dtrack.sh: |
    #!/bin/sh
    # Bootstrap Dependency-Track: change default password and extract API key.
    # Runs as a Job, writes the API key to a shared volume.
    set -e

    DT_URL="${DEPENDENCY_TRACK_URL:-http://dtrack-api:8080}"
    DT_ADMIN_USER="admin"
    DT_DEFAULT_PASS="admin"
    DT_NEW_PASS="${DEPENDENCY_TRACK_ADMIN_PASSWORD:-ArtifactKeeper2026!}"
    API_KEY_FILE="/shared/dtrack-api-key"
    BOOTSTRAP_MARKER="/shared/.dtrack-bootstrapped"

    echo "[dtrack-init] Waiting for Dependency-Track at $DT_URL ..."
    for i in $(seq 1 60); do
      if curl -sf "$DT_URL/api/version" > /dev/null 2>&1; then
        break
      fi
      if [ "$i" -eq 60 ]; then
        echo "[dtrack-init] ERROR: Dependency-Track did not become ready in 5 minutes"
        exit 1
      fi
      sleep 5
    done
    echo "[dtrack-init] Dependency-Track is up"

    # If API key file already exists from a previous run, skip all provisioning
    if [ -f "$API_KEY_FILE" ] && [ -s "$API_KEY_FILE" ]; then
      echo "[dtrack-init] API key already provisioned at $API_KEY_FILE -- skipping"
      exit 0
    fi

    # Try login with the new password first (already changed in a previous partial run)
    TOKEN=$(curl -sf -X POST "$DT_URL/api/v1/user/login" \
      -H "Content-Type: application/x-www-form-urlencoded" \
      -d "username=${DT_ADMIN_USER}&password=${DT_NEW_PASS}" 2>/dev/null || true)

    if [ -z "$TOKEN" ] || echo "$TOKEN" | grep -qi "FORCE_PASSWORD_CHANGE"; then
      # First boot: change the default admin password
      echo "[dtrack-init] Changing default admin password..."
      CHANGE_RESULT=$(curl -sf -o /dev/null -w "%{http_code}" \
        -X POST "$DT_URL/api/v1/user/forceChangePassword" \
        -H "Content-Type: application/x-www-form-urlencoded" \
        -d "username=${DT_ADMIN_USER}&password=${DT_DEFAULT_PASS}&newPassword=${DT_NEW_PASS}&confirmPassword=${DT_NEW_PASS}")

      if [ "$CHANGE_RESULT" != "200" ]; then
        echo "[dtrack-init] WARNING: Password change returned HTTP $CHANGE_RESULT (may already be changed)"
      fi

      # Login with new password
      TOKEN=$(curl -sf -X POST "$DT_URL/api/v1/user/login" \
        -H "Content-Type: application/x-www-form-urlencoded" \
        -d "username=${DT_ADMIN_USER}&password=${DT_NEW_PASS}" 2>/dev/null || true)
    fi

    if [ -z "$TOKEN" ]; then
      echo "[dtrack-init] ERROR: Could not authenticate with Dependency-Track"
      exit 1
    fi

    echo "[dtrack-init] Authenticated successfully"

    # Extract the Automation team's API key using jq
    API_KEY=$(curl -sf "$DT_URL/api/v1/team" \
      -H "Authorization: Bearer $TOKEN" | \
      jq -r '.[] | select(.name == "Automation") | .apiKeys[0].key // empty')

    if [ -z "$API_KEY" ]; then
      echo "[dtrack-init] ERROR: Could not find Automation team API key"
      echo "[dtrack-init] Available teams:"
      curl -sf "$DT_URL/api/v1/team" \
        -H "Authorization: Bearer $TOKEN" | jq -r '.[].name' 2>/dev/null || true
      exit 1
    fi

    echo "$API_KEY" > "$API_KEY_FILE"
    echo "[dtrack-init] API key written to $API_KEY_FILE"

    # Enable NVD API 2.0 mirroring
    echo "[dtrack-init] Enabling NVD API 2.0 vulnerability source..."
    NVD_RESULT=$(curl -sf -o /dev/null -w "%{http_code}" \
      -X POST "$DT_URL/api/v1/configProperty" \
      -H "Authorization: Bearer $TOKEN" \
      -H "Content-Type: application/json" \
      -d '{"groupName":"vuln-source","propertyName":"nvd.feeds.url","propertyValue":"https://services.nvd.nist.gov/rest/json/cves/2.0"}')
    if [ "$NVD_RESULT" = "200" ]; then
      echo "[dtrack-init] NVD API 2.0 source configured"
    else
      echo "[dtrack-init] WARNING: NVD config returned HTTP $NVD_RESULT (may already be set or unsupported)"
    fi

    echo "[dtrack-init] Done"

---
# =============================================================================
# PersistentVolume + PVC: PostgreSQL (10Gi)
# =============================================================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: artifactkeeper-1-postgres-pv
  labels:
    app: postgres
    instance: artifactkeeper-1
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /home/khan/ak-data/artifactkeeper-1/postgres
    type: DirectoryOrCreate

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-data
  namespace: artifactkeeper-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  volumeName: artifactkeeper-1-postgres-pv
  storageClassName: ""

---
# =============================================================================
# PersistentVolume + PVC: Meilisearch (5Gi)
# =============================================================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: artifactkeeper-1-meilisearch-pv
  labels:
    app: meilisearch
    instance: artifactkeeper-1
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /home/khan/ak-data/artifactkeeper-1/meilisearch
    type: DirectoryOrCreate

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: meilisearch-data
  namespace: artifactkeeper-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  volumeName: artifactkeeper-1-meilisearch-pv
  storageClassName: ""

---
# =============================================================================
# PersistentVolume + PVC: Trivy Cache (5Gi)
# =============================================================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: artifactkeeper-1-trivy-cache-pv
  labels:
    app: trivy
    instance: artifactkeeper-1
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /home/khan/ak-data/artifactkeeper-1/trivy-cache
    type: DirectoryOrCreate

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: trivy-cache
  namespace: artifactkeeper-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  volumeName: artifactkeeper-1-trivy-cache-pv
  storageClassName: ""

---
# =============================================================================
# PersistentVolume + PVC: DependencyTrack Data (5Gi)
# =============================================================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: artifactkeeper-1-dtrack-data-pv
  labels:
    app: dtrack
    instance: artifactkeeper-1
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /home/khan/ak-data/artifactkeeper-1/dtrack-data
    type: DirectoryOrCreate

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dtrack-data
  namespace: artifactkeeper-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  volumeName: artifactkeeper-1-dtrack-data-pv
  storageClassName: ""

---
# =============================================================================
# PersistentVolume + PVC: Artifact Storage (10Gi)
# =============================================================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: artifactkeeper-1-artifact-storage-pv
  labels:
    app: backend
    instance: artifactkeeper-1
    volume: artifact-storage
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /home/khan/ak-data/artifactkeeper-1/artifact-storage
    type: DirectoryOrCreate

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: artifact-storage
  namespace: artifactkeeper-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  volumeName: artifactkeeper-1-artifact-storage-pv
  storageClassName: ""

---
# =============================================================================
# PersistentVolume + PVC: Scan Workspace (2Gi, shared between backend + trivy)
# =============================================================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: artifactkeeper-1-scan-workspace-pv
  labels:
    app: shared
    instance: artifactkeeper-1
    volume: scan-workspace
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /home/khan/ak-data/artifactkeeper-1/scan-workspace
    type: DirectoryOrCreate

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: scan-workspace
  namespace: artifactkeeper-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  volumeName: artifactkeeper-1-scan-workspace-pv
  storageClassName: ""

---
# =============================================================================
# PersistentVolume + PVC: Shared Config (1Gi, shared between dtrack-init + backend)
# =============================================================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: artifactkeeper-1-shared-config-pv
  labels:
    app: shared
    instance: artifactkeeper-1
    volume: shared-config
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /home/khan/ak-data/artifactkeeper-1/shared-config
    type: DirectoryOrCreate

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-config
  namespace: artifactkeeper-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  volumeName: artifactkeeper-1-shared-config-pv
  storageClassName: ""

---
# =============================================================================
# PostgreSQL - StatefulSet
# =============================================================================
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: artifactkeeper-1
  labels:
    app: postgres
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:16-alpine
          ports:
            - containerPort: 5432
              name: postgres
          env:
            - name: POSTGRES_USER
              value: "registry"
            - name: POSTGRES_PASSWORD
              value: "registry"
            - name: POSTGRES_DB
              value: "artifact_registry"
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
            - name: init-db
              mountPath: /docker-entrypoint-initdb.d
              readOnly: true
          readinessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - registry
                - -d
                - artifact_registry
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - registry
                - -d
                - artifact_registry
            initialDelaySeconds: 15
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: postgres-data
        - name: init-db
          configMap:
            name: postgres-init-db

---
# =============================================================================
# PostgreSQL - Service
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: artifactkeeper-1
  labels:
    app: postgres
spec:
  type: ClusterIP
  ports:
    - port: 5432
      targetPort: 5432
      name: postgres
  selector:
    app: postgres

---
# =============================================================================
# Meilisearch - Deployment
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: meilisearch
  namespace: artifactkeeper-1
  labels:
    app: meilisearch
spec:
  replicas: 1
  selector:
    matchLabels:
      app: meilisearch
  template:
    metadata:
      labels:
        app: meilisearch
    spec:
      containers:
        - name: meilisearch
          image: getmeili/meilisearch:v1.12
          ports:
            - containerPort: 7700
              name: http
          env:
            - name: MEILI_MASTER_KEY
              value: "artifact-keeper-dev-key"
            - name: MEILI_ENV
              value: "development"
          volumeMounts:
            - name: meilisearch-data
              mountPath: /meili_data
          readinessProbe:
            httpGet:
              path: /health
              port: 7700
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 7700
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
      volumes:
        - name: meilisearch-data
          persistentVolumeClaim:
            claimName: meilisearch-data

---
# =============================================================================
# Meilisearch - Service
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: meilisearch
  namespace: artifactkeeper-1
  labels:
    app: meilisearch
spec:
  type: ClusterIP
  ports:
    - port: 7700
      targetPort: 7700
      name: http
  selector:
    app: meilisearch

---
# =============================================================================
# Trivy - Deployment
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trivy
  namespace: artifactkeeper-1
  labels:
    app: trivy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trivy
  template:
    metadata:
      labels:
        app: trivy
    spec:
      containers:
        - name: trivy
          image: aquasec/trivy:latest
          command: ["trivy"]
          args: ["server", "--listen", "0.0.0.0:8090"]
          ports:
            - containerPort: 8090
              name: http
          volumeMounts:
            - name: trivy-cache
              mountPath: /root/.cache/trivy
            - name: scan-workspace
              mountPath: /scan-workspace
              readOnly: true
          readinessProbe:
            tcpSocket:
              port: 8090
            initialDelaySeconds: 15
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          livenessProbe:
            tcpSocket:
              port: 8090
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
      volumes:
        - name: trivy-cache
          persistentVolumeClaim:
            claimName: trivy-cache
        - name: scan-workspace
          persistentVolumeClaim:
            claimName: scan-workspace

---
# =============================================================================
# Trivy - Service
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: trivy
  namespace: artifactkeeper-1
  labels:
    app: trivy
spec:
  type: ClusterIP
  ports:
    - port: 8090
      targetPort: 8090
      name: http
  selector:
    app: trivy

---
# =============================================================================
# DependencyTrack API Server - Deployment
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dtrack-api
  namespace: artifactkeeper-1
  labels:
    app: dtrack-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dtrack-api
  template:
    metadata:
      labels:
        app: dtrack-api
    spec:
      initContainers:
        - name: wait-for-postgres
          image: postgres:16-alpine
          command:
            - /bin/sh
            - -c
            - |
              echo "Waiting for PostgreSQL to be ready..."
              until pg_isready -h postgres -p 5432 -U registry; do
                echo "PostgreSQL not ready yet, retrying in 3s..."
                sleep 3
              done
              echo "PostgreSQL is ready"
      containers:
        - name: dtrack-api
          image: dependencytrack/apiserver:4.11.4
          ports:
            - containerPort: 8080
              name: http
          env:
            - name: ALPINE_DATABASE_MODE
              value: "external"
            - name: ALPINE_DATABASE_URL
              value: "jdbc:postgresql://postgres:5432/dependency_track"
            - name: ALPINE_DATABASE_DRIVER
              value: "org.postgresql.Driver"
            - name: ALPINE_DATABASE_USERNAME
              value: "registry"
            - name: ALPINE_DATABASE_PASSWORD
              value: "registry"
            - name: ALPINE_DATA_DIRECTORY
              value: "/data"
            - name: ALPINE_ENFORCE_AUTHENTICATION
              value: "true"
            - name: ALPINE_CORS_ENABLED
              value: "true"
            - name: ALPINE_CORS_ALLOW_ORIGIN
              value: "*"
            - name: JAVA_OPTIONS
              value: "-Xmx4g"
          volumeMounts:
            - name: dtrack-data
              mountPath: /data
          readinessProbe:
            exec:
              command:
                - wget
                - -q
                - --spider
                - http://localhost:8080/api/version
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5
          livenessProbe:
            exec:
              command:
                - wget
                - -q
                - --spider
                - http://localhost:8080/api/version
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5
          resources:
            requests:
              memory: "4Gi"
              cpu: "500m"
            limits:
              memory: "6Gi"
              cpu: "2000m"
      volumes:
        - name: dtrack-data
          persistentVolumeClaim:
            claimName: dtrack-data

---
# =============================================================================
# DependencyTrack API Server - Service
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: dtrack-api
  namespace: artifactkeeper-1
  labels:
    app: dtrack-api
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      name: http
  selector:
    app: dtrack-api

---
# =============================================================================
# DTrack Init - Job
# =============================================================================
apiVersion: batch/v1
kind: Job
metadata:
  name: dtrack-init
  namespace: artifactkeeper-1
  labels:
    app: dtrack-init
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: dtrack-init
    spec:
      restartPolicy: OnFailure
      containers:
        - name: dtrack-init
          image: alpine:3.20
          command:
            - /bin/sh
            - -c
            - |
              apk add --no-cache curl jq >/dev/null 2>&1
              /bin/sh /scripts/init-dtrack.sh
          env:
            - name: DEPENDENCY_TRACK_URL
              value: "http://dtrack-api:8080"
          volumeMounts:
            - name: init-script
              mountPath: /scripts
              readOnly: true
            - name: shared-config
              mountPath: /shared
      volumes:
        - name: init-script
          configMap:
            name: dtrack-init-script
            defaultMode: 0755
        - name: shared-config
          persistentVolumeClaim:
            claimName: shared-config

---
# =============================================================================
# Backend - Deployment
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: artifactkeeper-1
  labels:
    app: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      initContainers:
        - name: wait-for-postgres
          image: postgres:16-alpine
          command:
            - /bin/sh
            - -c
            - |
              echo "Waiting for PostgreSQL to be ready..."
              until pg_isready -h postgres -p 5432 -U registry; do
                echo "PostgreSQL not ready yet, retrying in 3s..."
                sleep 3
              done
              echo "PostgreSQL is ready"
        - name: wait-for-meilisearch
          image: alpine:3.20
          command:
            - /bin/sh
            - -c
            - |
              apk add --no-cache curl >/dev/null 2>&1
              echo "Waiting for Meilisearch to be ready..."
              until curl -sf http://meilisearch:7700/health > /dev/null 2>&1; do
                echo "Meilisearch not ready yet, retrying in 3s..."
                sleep 3
              done
              echo "Meilisearch is ready"
      containers:
        - name: backend
          image: ghcr.io/artifact-keeper/artifact-keeper-backend:dev
          imagePullPolicy: Always
          command:
            - /bin/sh
            - -c
            - |
              if [ -f /shared/dtrack-api-key ] && [ -s /shared/dtrack-api-key ]; then
                export DEPENDENCY_TRACK_API_KEY="$(cat /shared/dtrack-api-key)"
                echo "[backend] Loaded DTrack API key from shared config"
              else
                echo "[backend] No DTrack API key found at /shared/dtrack-api-key (dtrack-init may not have run yet)"
              fi
              exec /usr/local/bin/artifact-keeper
          ports:
            - containerPort: 8080
              name: http
            - containerPort: 9090
              name: grpc
          env:
            - name: DATABASE_URL
              value: "postgresql://registry:registry@postgres:5432/artifact_registry"
            - name: STORAGE_PATH
              value: "/data/storage"
            - name: BACKUP_PATH
              value: "/data/backups"
            - name: PLUGINS_DIR
              value: "/data/plugins"
            - name: JWT_SECRET
              value: "dev-secret-change-in-production"
            - name: ADMIN_PASSWORD
              value: "admin"
            - name: RUST_LOG
              value: "info,artifact_keeper=debug"
            - name: ENVIRONMENT
              value: "development"
            - name: TRIVY_URL
              value: "http://trivy:8090"
            - name: SCAN_WORKSPACE_PATH
              value: "/scan-workspace"
            - name: MEILISEARCH_URL
              value: "http://meilisearch:7700"
            - name: MEILISEARCH_API_KEY
              value: "artifact-keeper-dev-key"
            - name: DEPENDENCY_TRACK_URL
              value: "http://dtrack-api:8080"
            - name: DEPENDENCY_TRACK_ENABLED
              value: "true"
            - name: HOST
              value: "0.0.0.0"
            - name: PORT
              value: "8080"
          volumeMounts:
            - name: artifact-storage
              mountPath: /data/storage
              subPath: storage
            - name: artifact-storage
              mountPath: /data/backups
              subPath: backups
            - name: artifact-storage
              mountPath: /data/plugins
              subPath: plugins
            - name: scan-workspace
              mountPath: /scan-workspace
            - name: shared-config
              mountPath: /shared
              readOnly: true
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 5
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "2000m"
      volumes:
        - name: artifact-storage
          persistentVolumeClaim:
            claimName: artifact-storage
        - name: scan-workspace
          persistentVolumeClaim:
            claimName: scan-workspace
        - name: shared-config
          persistentVolumeClaim:
            claimName: shared-config

---
# =============================================================================
# Backend - Service (ClusterIP, fronted by Caddy ingress)
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: artifactkeeper-1
  labels:
    app: backend
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      name: http
    - port: 9090
      targetPort: 9090
      name: grpc
  selector:
    app: backend

---
# =============================================================================
# Web Frontend - Deployment
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
  namespace: artifactkeeper-1
  labels:
    app: web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: web
          image: ghcr.io/artifact-keeper/artifact-keeper-web:dev
          imagePullPolicy: Always
          ports:
            - containerPort: 3000
              name: http
          env:
            - name: NEXT_PUBLIC_API_URL
              value: ""
            - name: BACKEND_URL
              value: "http://backend:8080"
            - name: NODE_ENV
              value: "production"
          readinessProbe:
            httpGet:
              path: /
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          livenessProbe:
            httpGet:
              path: /
              port: 3000
            initialDelaySeconds: 20
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 5
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "1000m"

---
# =============================================================================
# Web Frontend - Service (ClusterIP, fronted by Caddy ingress)
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: web
  namespace: artifactkeeper-1
  labels:
    app: web
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: 3000
      name: http
  selector:
    app: web

---
# =============================================================================
# Caddy Ingress - ConfigMap (Caddyfile)
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: caddy-config
  namespace: artifactkeeper-1
data:
  Caddyfile: |
    {
      auto_https off
    }

    :80 {
      # Backend API - direct access for mobile/CLI clients
      handle /api/* {
        reverse_proxy backend:8080
      }

      # Health endpoint
      handle /health {
        reverse_proxy backend:8080
      }

      # DependencyTrack UI and API
      handle /dtrack/* {
        uri strip_prefix /dtrack
        reverse_proxy dtrack-api:8080
      }

      # Everything else goes to the web frontend
      handle {
        reverse_proxy web:3000
      }
    }

    # gRPC port (HTTP/2 cleartext, TLS handled by Tailscale)
    :9090 {
      reverse_proxy {
        to h2c://backend:9090
      }
    }

---
# =============================================================================
# Caddy Ingress - Deployment
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: caddy
  namespace: artifactkeeper-1
  labels:
    app: caddy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: caddy
  template:
    metadata:
      labels:
        app: caddy
    spec:
      containers:
        - name: caddy
          image: caddy:2-alpine
          ports:
            - containerPort: 80
              name: http
            - containerPort: 9090
              name: grpc
          volumeMounts:
            - name: caddy-config
              mountPath: /etc/caddy/Caddyfile
              subPath: Caddyfile
              readOnly: true
            - name: caddy-data
              mountPath: /data
            - name: caddy-config-dir
              mountPath: /config
          readinessProbe:
            tcpSocket:
              port: 80
            initialDelaySeconds: 5
            periodSeconds: 10
            failureThreshold: 3
          livenessProbe:
            tcpSocket:
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 15
            failureThreshold: 3
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "256Mi"
              cpu: "500m"
      volumes:
        - name: caddy-config
          configMap:
            name: caddy-config
        - name: caddy-data
          emptyDir: {}
        - name: caddy-config-dir
          emptyDir: {}

---
# =============================================================================
# Caddy Ingress - Service (NodePort - single entry point)
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: caddy
  namespace: artifactkeeper-1
  labels:
    app: caddy
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30080
      name: http
    - port: 9090
      targetPort: 9090
      nodePort: 30090
      name: grpc
  selector:
    app: caddy
