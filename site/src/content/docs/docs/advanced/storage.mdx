---
title: "Storage Backends"
description: "Filesystem and S3-compatible storage options for artifact data"
---

Artifact Keeper supports multiple storage backends to accommodate different deployment scenarios and scale requirements.

## Storage Backend Types

### Filesystem Storage

The default backend stores artifacts on the local filesystem or network-attached storage.

#### Configuration

```bash
STORAGE_BACKEND=filesystem
STORAGE_PATH=/var/lib/artifact-keeper/artifacts
```

#### Advantages

- Simple setup, no external dependencies
- Predictable performance
- Easy to backup with standard tools
- Works well with NFS/NAS for shared storage

#### Limitations

- Scaling requires network storage
- No built-in redundancy
- Manual backup procedures

#### Directory Structure

```text
/var/lib/artifact-keeper/artifacts/
├── repositories/
│   ├── repo-{id}/
│   │   ├── packages/
│   │   │   ├── {package-name}/
│   │   │   │   ├── {version}/
│   │   │   │   │   ├── {artifact-file}
│   │   │   │   │   └── metadata.json
├── temp/           # Temporary upload staging
└── cache/          # Downloaded edge cache
```

#### Permissions

Ensure the backend process has read/write access:

```bash
sudo mkdir -p /var/lib/artifact-keeper/artifacts
sudo chown -R artifact-keeper:artifact-keeper /var/lib/artifact-keeper
sudo chmod -R 750 /var/lib/artifact-keeper/artifacts
```

### S3-Compatible Storage

Use Amazon S3 or compatible object storage (MinIO, Wasabi, DigitalOcean Spaces, etc.) for cloud-native deployments.

#### Configuration

```bash
STORAGE_BACKEND=s3
S3_BUCKET=artifact-keeper-prod
S3_REGION=us-east-1
S3_ENDPOINT=https://s3.amazonaws.com  # Optional, for S3-compatible services
S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
S3_PATH_PREFIX=artifacts/  # Optional, prefix for all keys
```

#### AWS S3

```bash
STORAGE_BACKEND=s3
S3_BUCKET=my-artifact-bucket
S3_REGION=us-west-2
# Use IAM roles for credentials (recommended)
# Or set S3_ACCESS_KEY_ID and S3_SECRET_ACCESS_KEY
```

#### MinIO (Self-Hosted)

```bash
STORAGE_BACKEND=s3
S3_BUCKET=artifacts
S3_REGION=us-east-1
S3_ENDPOINT=https://minio.example.com
S3_ACCESS_KEY_ID=minioadmin
S3_SECRET_ACCESS_KEY=minioadmin
S3_FORCE_PATH_STYLE=true  # Required for MinIO
```

#### DigitalOcean Spaces

```bash
STORAGE_BACKEND=s3
S3_BUCKET=my-spaces-bucket
S3_REGION=nyc3
S3_ENDPOINT=https://nyc3.digitaloceanspaces.com
S3_ACCESS_KEY_ID=your-spaces-key
S3_SECRET_ACCESS_KEY=your-spaces-secret
```

#### Advantages

- Unlimited scalability
- Built-in redundancy and durability
- Geographic distribution
- No filesystem management
- Pay-as-you-go pricing

#### Considerations

- Network latency for uploads/downloads
- Data transfer costs
- Requires internet connectivity
- Credential management

### S3 Direct Downloads (302 Redirects)

By default, artifact downloads are proxied through the Artifact Keeper backend. For S3 storage, you can enable direct downloads using presigned URLs, which:

- **Reduces backend load** - clients download directly from S3
- **Lowers egress costs** - no data transfer through your backend servers
- **Improves performance** - S3's global infrastructure serves content faster
- **Scales infinitely** - no backend bottleneck for downloads

#### Configuration

```bash
STORAGE_BACKEND=s3
S3_BUCKET=artifact-keeper-prod
S3_REGION=us-east-1
S3_REDIRECT_DOWNLOADS=true        # Enable 302 redirects
S3_PRESIGN_EXPIRY=3600            # URL expiry in seconds (default: 1 hour)
```

#### How It Works

1. Client requests artifact download from Artifact Keeper
2. Backend generates a presigned S3 URL (valid for `S3_PRESIGN_EXPIRY` seconds)
3. Backend returns `302 Found` with `Location` header pointing to S3
4. Client downloads directly from S3 using the presigned URL

```text
Client                    Artifact Keeper              S3
  |                              |                      |
  |-- GET /artifacts/pkg.tar --> |                      |
  |                              |-- Generate URL -->   |
  |<-- 302 Location: s3://... -- |                      |
  |                                                     |
  |------------------- GET s3://... -----------------> |
  |<------------------ [file content] ---------------- |
```

#### Response Headers

When redirect is enabled, responses include:

```http
HTTP/1.1 302 Found
Location: https://bucket.s3.amazonaws.com/path?X-Amz-Signature=...
X-Artifact-Storage: redirect-s3
Cache-Control: private, max-age=3600
```

The `X-Artifact-Storage` header indicates how the artifact was served:
- `redirect-s3` - Redirected to S3 presigned URL
- `redirect-cloudfront` - Redirected to CloudFront signed URL
- `proxy` - Streamed through backend (filesystem or redirect disabled)

#### Security Considerations

- Presigned URLs grant temporary access to the specific object
- URLs expire after `S3_PRESIGN_EXPIRY` seconds
- Authentication is still enforced at the backend before generating URLs
- S3 bucket should block public access; only presigned URLs work

#### Presigned URL Credentials & STS Roles

When using IAM roles (EC2, ECS, Lambda), AWS credentials are temporary (STS) and expire periodically. A presigned URL's effective lifetime is limited by the remaining lifetime of the credential that generated it — not just the configured expiry. For example, if your STS credentials expire in 60 seconds and you generate a 1-hour presigned URL, the URL is only valid for 60 seconds.

Artifact Keeper automatically refreshes credentials before generating each presigned URL (Option A), ensuring URLs get the full requested lifetime. No configuration needed — this works transparently with any credential source.

For maximum reliability, you can also configure dedicated long-lived IAM credentials specifically for URL signing (Option B):

```bash
S3_PRESIGN_ACCESS_KEY_ID=AKIAEXAMPLE...
S3_PRESIGN_SECRET_ACCESS_KEY=wJalrXUtn...
```

The dedicated IAM user only needs `s3:GetObject` permission on your bucket. When set, these credentials are used exclusively for signing presigned URLs, avoiding any STS credential lifetime issues entirely.

| Approach | Pros | Cons |
|----------|------|------|
| Auto-refresh (default) | Zero config, works with any credential source | Adds ~1 metadata API call per redirect |
| Dedicated credentials | No expiry concerns, predictable | Requires managing a separate IAM user |

### CloudFront Integration

For global distribution and even faster downloads, use CloudFront with signed URLs.

#### Why CloudFront?

- **Edge caching** - Popular artifacts served from nearby edge locations
- **Lower latency** - Global CDN network
- **Reduced S3 costs** - Cache hits don't incur S3 request fees
- **DDoS protection** - AWS Shield integration

#### Configuration

```bash
STORAGE_BACKEND=s3
S3_BUCKET=artifact-keeper-prod
S3_REGION=us-east-1
S3_REDIRECT_DOWNLOADS=true

# CloudFront settings
CLOUDFRONT_DISTRIBUTION_URL=https://d1234567890.cloudfront.net
CLOUDFRONT_KEY_PAIR_ID=APKAEIBAERJR2EXAMPLE
CLOUDFRONT_PRIVATE_KEY_PATH=/etc/artifact-keeper/cloudfront-private-key.pem
# Or inline:
# CLOUDFRONT_PRIVATE_KEY="-----BEGIN RSA PRIVATE KEY-----\n..."
```

#### CloudFront Setup

1. **Create CloudFront Distribution**
   - Origin: Your S3 bucket
   - Origin Access Control (OAC): Restrict bucket access to CloudFront only
   - Viewer Protocol Policy: Redirect HTTP to HTTPS

2. **Create CloudFront Key Pair**
   ```bash
   # Generate RSA key pair
   openssl genrsa -out cloudfront-private-key.pem 2048
   openssl rsa -in cloudfront-private-key.pem -pubout -out cloudfront-public-key.pem
   ```

3. **Upload Public Key to CloudFront**
   - Go to CloudFront → Key Management → Public Keys
   - Create public key with your `cloudfront-public-key.pem`
   - Note the Key Pair ID (e.g., `APKAEIBAERJR2EXAMPLE`)

4. **Create Key Group**
   - Go to CloudFront → Key Management → Key Groups
   - Create key group with your public key

5. **Configure Distribution Behavior**
   - Restrict Viewer Access: Yes
   - Trusted Key Groups: Select your key group

6. **Update S3 Bucket Policy**
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Principal": {
           "Service": "cloudfront.amazonaws.com"
         },
         "Action": "s3:GetObject",
         "Resource": "arn:aws:s3:::artifact-keeper-prod/*",
         "Condition": {
           "StringEquals": {
             "AWS:SourceArn": "arn:aws:cloudfront::123456789012:distribution/EDFDVBD6EXAMPLE"
           }
         }
       }
     ]
   }
   ```

#### CloudFront Response Headers

When CloudFront is configured, downloads redirect to the CDN:

```http
HTTP/1.1 302 Found
Location: https://d1234567890.cloudfront.net/path?Policy=...&Signature=...&Key-Pair-Id=...
X-Artifact-Storage: redirect-cloudfront
Cache-Control: private, max-age=3600
```

#### Fallback Behavior

If CloudFront configuration is incomplete or fails:
1. Falls back to S3 presigned URLs (if S3 redirect enabled)
2. Falls back to proxied download (if all else fails)

### Disabling Redirects

To proxy all downloads through the backend (useful for auditing or custom headers):

```bash
S3_REDIRECT_DOWNLOADS=false
```

All downloads will stream through the backend with `X-Artifact-Storage: proxy`.

## MinIO as S3 Alternative

MinIO provides S3-compatible object storage that you can self-host.

### Docker Compose Setup

```yaml
version: '3.8'
services:
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  artifact-keeper:
    image: artifact-keeper-backend:latest
    environment:
      STORAGE_BACKEND: s3
      S3_BUCKET: artifacts
      S3_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY_ID: minioadmin
      S3_SECRET_ACCESS_KEY: minioadmin
      S3_FORCE_PATH_STYLE: "true"
    depends_on:
      - minio

volumes:
  minio_data:
```

### Create Bucket

Access MinIO console at http://localhost:9001 and create the `artifacts` bucket, or use the CLI:

```bash
mc alias set local http://localhost:9000 minioadmin minioadmin
mc mb local/artifacts
mc policy set download local/artifacts  # Optional: public read
```

## Storage Layout

Regardless of backend, artifacts are organized hierarchically:

### Key/Path Structure

```text
{repository_id}/packages/{package_name}/{version}/{artifact_filename}
```

Examples:

```text
repo-123/packages/my-app/1.0.0/my-app-1.0.0.tar.gz
repo-456/packages/@scope/package/2.1.3/package-2.1.3.tgz
repo-789/packages/my-image/latest/manifest.json
```

### Metadata Storage

Artifact metadata is stored in PostgreSQL, not in the storage backend. The storage backend only contains the binary artifact files.

## Garbage Collection

Remove orphaned artifacts that are no longer referenced in the database.

### Manual Cleanup

```bash
curl -X POST https://registry.example.com/api/v1/admin/cleanup \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "dry_run": true,
    "older_than_days": 30
  }'
```

### Scheduled Cleanup

Configure automatic garbage collection:

```bash
GC_ENABLED=true
GC_SCHEDULE="0 2 * * *"  # Daily at 2 AM
GC_RETENTION_DAYS=90     # Keep artifacts for 90 days
```

### What Gets Cleaned

- Artifacts marked as deleted but still on disk
- Incomplete multipart uploads (>24 hours old)
- Temporary files from failed uploads
- Orphaned chunks from interrupted edge transfers

### Dry Run Mode

Always test with dry run first:

```bash
GC_DRY_RUN=true
```

This logs what would be deleted without actually removing anything.

## Storage Migration

### From Filesystem to S3

1. Configure S3 backend settings
2. Run migration tool:

```bash
cargo run --bin migrate-storage -- \
  --from filesystem \
  --from-path /var/lib/artifact-keeper/artifacts \
  --to s3 \
  --s3-bucket artifact-keeper-prod
```

3. Verify migration:

```bash
cargo run --bin migrate-storage -- --verify
```

4. Update backend configuration to use S3
5. Restart backend services

### From S3 to Filesystem

Same process in reverse:

```bash
cargo run --bin migrate-storage -- \
  --from s3 \
  --s3-bucket artifact-keeper-prod \
  --to filesystem \
  --to-path /var/lib/artifact-keeper/artifacts
```

## Performance Tuning

### Filesystem

```bash
# Use faster filesystem for metadata
STORAGE_PATH=/mnt/ssd/artifacts

# Enable direct I/O for large files
STORAGE_DIRECT_IO=true

# Adjust buffer sizes
STORAGE_BUFFER_SIZE=1048576  # 1 MB
```

### S3

```bash
# Multipart upload threshold
S3_MULTIPART_THRESHOLD=104857600  # 100 MB

# Chunk size for multipart uploads
S3_MULTIPART_CHUNK_SIZE=10485760  # 10 MB

# Connection pooling
S3_MAX_CONNECTIONS=50

# Enable transfer acceleration (AWS S3 only)
S3_USE_TRANSFER_ACCELERATION=true
```

## Backup Considerations

### Filesystem Backend

Use standard backup tools:

```bash
# rsync to backup location
rsync -av /var/lib/artifact-keeper/artifacts/ /backup/artifacts/

# Tar archive
tar -czf artifacts-backup.tar.gz /var/lib/artifact-keeper/artifacts
```

### S3 Backend

Enable versioning and lifecycle policies:

```bash
# Enable versioning
aws s3api put-bucket-versioning \
  --bucket artifact-keeper-prod \
  --versioning-configuration Status=Enabled

# Lifecycle rule for old versions
aws s3api put-bucket-lifecycle-configuration \
  --bucket artifact-keeper-prod \
  --lifecycle-configuration file://lifecycle.json
```

Use S3 replication for disaster recovery:

```bash
aws s3api put-bucket-replication \
  --bucket artifact-keeper-prod \
  --replication-configuration file://replication.json
```

## Monitoring

### Storage Metrics

Monitor these metrics:

- Total storage size
- Number of artifacts
- Upload/download throughput
- Error rates (failed uploads/downloads)
- Storage backend latency

### Prometheus Metrics

```text
artifact_keeper_storage_size_bytes
artifact_keeper_storage_objects_total
artifact_keeper_storage_upload_duration_seconds
artifact_keeper_storage_download_duration_seconds
artifact_keeper_storage_errors_total
```

### Health Checks

```bash
# Check storage backend connectivity
curl https://registry.example.com/api/v1/admin/health/storage
```

## Troubleshooting

### Filesystem Permission Errors

```bash
# Check ownership
ls -la /var/lib/artifact-keeper/artifacts

# Fix permissions
sudo chown -R artifact-keeper:artifact-keeper /var/lib/artifact-keeper
```

### S3 Connection Issues

```bash
# Test credentials with AWS CLI
aws s3 ls s3://artifact-keeper-prod --profile your-profile

# Verify endpoint connectivity
curl -v https://s3.us-east-1.amazonaws.com
```

### High Storage Costs

- Enable garbage collection
- Set retention policies
- Use S3 lifecycle rules to move to cheaper storage classes
- Compress artifacts before upload
- Deduplicate using content-addressable storage
