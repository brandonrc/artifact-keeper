---
title: Architecture
description: "How Artifact Keeper is built: system design, technology choices, and the reasoning behind them."
---

Artifact Keeper is designed as a modular, layered system. This page walks through every major component so you can evaluate whether the architecture fits your needs and contribute with full context.

## System Overview

```mermaid
graph LR
    Client["Browser / CLI<br/>Package Manager"]
    Frontend["Frontend<br/>React 19 · Ant Design 6<br/>TanStack Query 5"]
    Backend["Backend<br/>Rust · Axum<br/>45+ format handlers"]
    DB[(PostgreSQL 16)]
    Storage["Storage<br/>Filesystem / S3"]
    Meili["Meilisearch<br/>Full-text search"]
    Trivy["Trivy<br/>Container & FS scanning"]
    Grype["Grype<br/>Dependency scanning"]
    Peer1["Peer Instance"]
    Peer2["Peer Instance"]

    Client --> Frontend
    Client --> Backend
    Frontend --> Backend
    Backend --> DB
    Backend --> Storage
    Backend --> Meili
    Backend --> Trivy
    Backend --> Grype
    Backend <-->|Borg Replication| Peer1
    Backend <-->|Borg Replication| Peer2
    Peer1 <-->|P2P Mesh| Peer2
```

**Five core services** make up a full deployment:

| Service | Role | Required? |
|---------|------|-----------|
| **Backend** (Rust/Axum) | API server, format handlers, business logic | Yes |
| **PostgreSQL 16** | Metadata, users, scan results, configuration | Yes |
| **Frontend** (React 19) | Web UI served via Nginx | Optional (API-first) |
| **Meilisearch** | Full-text search across artifacts and repos | Optional |
| **Trivy** | Vulnerability scanning for uploaded artifacts | Optional |

Meilisearch, Trivy, and Grype degrade gracefully — the system works without them, just without search or scanning.

## Backend Layers

Every HTTP request flows through the same pipeline:

```mermaid
flowchart TD
    REQ["HTTP Request"] --> MW

    subgraph MW["Middleware Pipeline"]
        direction LR
        CORS["CORS"] --> AUTH["Auth<br/>JWT · OIDC · LDAP<br/>SAML · API Key"]
        AUTH --> RL["Rate Limiter"]
        RL --> TRACE["Tracing<br/>+ Metrics"]
        TRACE --> DEMO["Demo Mode<br/>Guard"]
    end

    MW --> ROUTER["Router · 50+ route groups"]

    subgraph HANDLERS["Handler Layer"]
        FMT["Format Handlers<br/>Maven · PyPI · NPM<br/>Docker · 41 more"]
        CORE["Core Handlers<br/>Repos · Artifacts<br/>Users · Auth"]
        ADV["Advanced Handlers<br/>Security · Plugins<br/>Peers · Migration"]
    end

    ROUTER --> HANDLERS

    subgraph SERVICES["Service Layer"]
        direction LR
        ART["Artifact<br/>Service"]
        REPO["Repository<br/>Service"]
        SCAN["Scanner<br/>Service"]
        PLUG["Plugin<br/>Service"]
        SEARCH["Search<br/>Service"]
    end

    HANDLERS --> SERVICES

    subgraph DATA["Data Layer"]
        direction LR
        PG[(PostgreSQL)]
        FS["Storage<br/>FS / S3"]
        MS["Meilisearch"]
        SC["Trivy / Grype"]
    end

    SERVICES --> DATA
```

### Handlers

Handlers are thin HTTP endpoints. They validate input, call services, and format responses. There are three categories:

- **Format handlers** — One per package format (Maven, PyPI, NPM, Docker/OCI, etc.). Each speaks the native wire protocol that `pip install`, `npm publish`, `docker push`, etc. expect.
- **Core handlers** — Repositories, artifacts, users, auth, search, tree browsing.
- **Advanced handlers** — Security scanning, WASM plugins, peer replication, migration, webhooks, signing.

### Services

Services contain the business logic. They are injected into handlers via Axum's `State` extractor.

| Service | Responsibility |
|---------|---------------|
| `ArtifactService` | Upload, download, versioning, metadata extraction |
| `RepositoryService` | CRUD, virtual repo resolution, proxy upstream |
| `AuthService` | JWT validation, password hashing, token refresh |
| `ScannerService` | Orchestrates Trivy and Grype, aggregates findings |
| `MeiliService` | Index management, search queries |
| `WasmPluginService` | Plugin install, enable, disable, hot reload |
| `PeerService` | Peer registration, replication coordination |
| `MigrationService` | Artifactory import orchestration |
| `StorageService` | Abstraction over filesystem and S3 backends |
| `SigningService` | GPG/RSA signing for Debian, RPM, Alpine, Conda |

### Storage

Storage is trait-based with two backends:

```
StorageBackend trait
├── FilesystemStorage   (default, content-addressed by SHA-256)
└── S3Backend           (AWS S3, MinIO, or any S3-compatible service)
```

Artifacts are stored by their SHA-256 hash, enabling automatic deduplication across repositories. The backend chosen via `STORAGE_BACKEND` env var (`filesystem` or `s3`).

### Database

PostgreSQL 16 with 33 migrations covering:

- **Core**: users, roles, repositories, artifacts, artifact_metadata, download_statistics
- **Auth**: api_tokens, groups, permissions
- **Plugins**: plugins, plugin_hooks, plugin_events, plugin_config, format_handlers
- **Security**: scan_configs, scan_results, findings, security_scores, signing_keys
- **Replication**: peers, peer_connections, transfer_sessions, network_profiles, sync_tasks
- **Operations**: audit_log, webhooks, system_settings, backups, migration_jobs

All queries are compile-time checked via SQLx.

## Security Pipeline

```mermaid
flowchart LR
    UP["Artifact<br/>Upload"] --> HASH{"SHA-256<br/>Dedup"}
    HASH -->|New artifact| T["Trivy<br/>FS Scanner"]
    HASH -->|New artifact| G["Grype<br/>Dependency Scanner"]
    HASH -->|Already scanned| CACHE["Cached<br/>Results"]
    T --> SCORE["Vulnerability<br/>Score (A-F)"]
    G --> SCORE
    CACHE --> SCORE
    SCORE --> POL{"Policy<br/>Engine"}
    POL -->|Pass| OK["Stored"]
    POL -->|Fail| Q["Quarantined"]
```

- **Trivy** scans filesystem-based artifacts (wheels, JARs, tarballs, crates) by extracting them into a shared workspace.
- **Grype** performs dependency-tree analysis using SBOMs.
- **Deduplication** — If the SHA-256 hash was already scanned, cached results are reused.
- **Scoring** — Findings are weighted by severity and summed into an A-F grade.
- **Policies** — Admins configure rules (e.g., "block Critical", "quarantine High") per repository or globally.
- **Signing** — GPG and RSA keys can be used to sign repository metadata (Debian Release files, RPM repomd, Alpine APKINDEX, Conda repodata).

## Authentication

Multiple auth providers can be active simultaneously:

```mermaid
flowchart LR
    REQ["Request"] --> EXT{"Extract<br/>Credentials"}
    EXT -->|"Authorization: Bearer"| JWT["JWT<br/>Validation"]
    EXT -->|"Authorization: ApiKey"| API["API Token<br/>Lookup"]
    EXT -->|"X-API-Key"| API

    JWT --> USER["User Context"]
    API --> USER

    subgraph PROVIDERS["Identity Providers"]
        direction TB
        LOCAL["Local<br/>(bcrypt)"]
        OIDC["OpenID<br/>Connect"]
        LDAP["LDAP<br/>Directory"]
        SAML["SAML 2.0<br/>SSO"]
    end

    USER -.->|"Authenticated via"| PROVIDERS
```

- **Local auth** — bcrypt-hashed passwords stored in PostgreSQL
- **OIDC** — Federate with any OpenID Connect provider (Keycloak, Auth0, Okta, etc.)
- **LDAP** — Bind against Active Directory or OpenLDAP
- **SAML 2.0** — Enterprise SSO integration
- **API tokens** — Scoped, expiring tokens for CI/CD and automation
- **Rate limiting** — Per-IP and per-user with configurable windows

## Borg Replication

```mermaid
graph TD
    P1["Peer<br/>US-West"]
    P2["Peer<br/>EU-Central"]
    P3["Peer<br/>AP-Southeast"]
    P4["Peer<br/>US-East"]

    P1 <-->|"Chunked Transfer"| P2
    P1 <-->|"Chunked Transfer"| P4
    P2 <-->|"Chunked Transfer"| P3
    P3 <-->|"Chunked Transfer"| P4
    P1 <-->|"P2P Mesh"| P3
    P2 <-->|"P2P Mesh"| P4
```

The replication system distributes artifacts across a recursive peer mesh:

- **Recursive peers** — Every peer is a full Artifact Keeper instance (backend, DB, storage) that can originate replication to other peers.
- **P2P mesh** — Peers transfer chunks between each other, with no single hub bottleneck.
- **Chunked transfers** — Large artifacts are split into chunks for reliable delivery.
- **Network-aware scheduling** — Bandwidth and latency profiles determine transfer priority.
- **Replication priorities** — Immediate, Scheduled, OnDemand, or LocalOnly per repository.

## WASM Plugin System

Custom format handlers can be written in any language that compiles to WASM (Rust, Go, C, AssemblyScript, etc.):

```mermaid
flowchart LR
    SRC["Plugin Source<br/>Git / ZIP / Local"] --> MANIFEST["Manifest<br/>Validation"]
    MANIFEST --> COMPILE["Wasmtime<br/>Load & Link"]
    COMPILE --> REG["Plugin<br/>Registry"]
    REG --> FMT["FormatHandler<br/>Trait"]

    subgraph SANDBOX["Sandbox Limits"]
        direction TB
        CPU["Fuel-based<br/>CPU limit"]
        MEM["64 MB<br/>Memory cap"]
        TIME["Configurable<br/>Timeout"]
    end

    COMPILE -.-> SANDBOX
```

- **WIT interface** — Plugins implement a `FormatHandler` contract defined in WebAssembly Interface Types
- **Capabilities** — `parse_metadata`, `validate_artifact`, `generate_index` (each optional)
- **Lifecycle** — Install, enable, disable, reload — all without restarting the server
- **Sandboxing** — Fuel-based CPU metering, memory limits, and execution timeouts via Wasmtime

## Technology Choices

| Layer | Choice | Why |
|-------|--------|-----|
| Backend language | **Rust** | Memory safety without GC, predictable performance, strong type system |
| Web framework | **Axum** | Tower middleware ecosystem, async-first, modular extractors |
| Database | **PostgreSQL 16** | JSONB for flexible metadata, mature tooling, compile-time checked queries via SQLx |
| Frontend | **React 19 + TypeScript** | Component model, widespread ecosystem, type safety |
| UI library | **Ant Design 6** | Enterprise-grade components (tables, forms, modals) out of the box |
| Search | **Meilisearch** | Sub-50ms full-text search, simple to operate, typo tolerance |
| Security scanning | **Trivy + Grype** | Complementary coverage (filesystem + dependency), industry standard, free |
| Plugin runtime | **Wasmtime** | Sandboxed execution, WIT contracts, broad language support |
| Storage | **Filesystem / S3** | Simple default for single-node, S3 for cloud-native deployments |
| CI/CD | **GitHub Actions** | Native integration, matrix builds, reusable workflows |

## Project Structure

```
artifact-keeper/
├── backend/            # Rust backend
│   ├── src/
│   │   ├── api/        # Handlers (50+), middleware, routes
│   │   ├── formats/    # 45+ format handler implementations
│   │   ├── services/   # Business logic (35+ services)
│   │   ├── models/     # Data models (18 types)
│   │   └── storage/    # FS and S3 backends
│   └── migrations/     # 33 PostgreSQL migrations
├── frontend/           # React 19 + TypeScript frontend
│   ├── src/
│   │   ├── pages/      # 29 page components
│   │   ├── components/ # Reusable UI components
│   │   ├── api/        # API client modules
│   │   └── contexts/   # Auth and Theme providers
│   └── e2e/            # Playwright E2E tests
├── edge/               # Peer replication service (Rust)
├── site/               # Documentation (Astro + Starlight)
├── specs/              # Feature specifications
├── scripts/            # Test runners and utilities
├── deploy/             # Docker, Kubernetes, demo configs
└── .github/            # CI/CD workflows
```

## What's Next?

- [Quickstart](/docs/getting-started/quickstart/) — Get a local instance running in 5 minutes
- [Package Formats](/docs/package-formats/) — Full list of supported formats with client setup guides
- [REST API Reference](/docs/reference/api/) — Complete API documentation
